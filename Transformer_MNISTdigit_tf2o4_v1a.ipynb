{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da6ab406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9baccef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to change the MNIST label dimension\n",
    "def change_to_right(wrong_labels):\n",
    "    right_labels=[]\n",
    "    for x in wrong_labels:\n",
    "        for i in range(0,len(wrong_labels[0])):\n",
    "            if x[i]==1:\n",
    "                right_labels.append(i)\n",
    "    return right_labels\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# Convert class vectors to right format\n",
    "y_train = tf.convert_to_tensor(np.array(change_to_right(keras.utils.to_categorical(y_train, num_classes))))\n",
    "y_test = tf.convert_to_tensor(np.array(change_to_right(keras.utils.to_categorical(y_test, num_classes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1de083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28  # We'll resize input images to this size\n",
    "patch_size = 14  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 96\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 16\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179e0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.experimental.preprocessing.Normalization(),\n",
    "        #layers.Resizing(image_size, image_size),\n",
    "        #layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
    "        tf.keras.layers.experimental.preprocessing.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24462ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = inputs #data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930da2bc",
   "metadata": {},
   "source": [
    "##### tf2.4.0: https://docs.w3cub.com/tensorflow~2.4/keras/optimizers/adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7faac6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "27/27 [==============================] - 154s 5s/step - loss: 1.8277 - accuracy: 0.4197 - top-5-accuracy: 0.7971 - val_loss: 0.2655 - val_accuracy: 0.9163 - val_top-5-accuracy: 0.9960\n",
      "Epoch 2/5\n",
      "27/27 [==============================] - 151s 6s/step - loss: 0.3778 - accuracy: 0.8838 - top-5-accuracy: 0.9928 - val_loss: 0.1482 - val_accuracy: 0.9578 - val_top-5-accuracy: 0.9980\n",
      "Epoch 3/5\n",
      "27/27 [==============================] - 164s 6s/step - loss: 0.2190 - accuracy: 0.9334 - top-5-accuracy: 0.9969 - val_loss: 0.1094 - val_accuracy: 0.9692 - val_top-5-accuracy: 0.9982\n",
      "Epoch 4/5\n",
      "27/27 [==============================] - 150s 6s/step - loss: 0.1572 - accuracy: 0.9513 - top-5-accuracy: 0.9983 - val_loss: 0.0888 - val_accuracy: 0.9762 - val_top-5-accuracy: 0.9985\n",
      "Epoch 5/5\n",
      "27/27 [==============================] - 161s 6s/step - loss: 0.1334 - accuracy: 0.9607 - top-5-accuracy: 0.9983 - val_loss: 0.0873 - val_accuracy: 0.9773 - val_top-5-accuracy: 0.9985\n",
      "313/313 [==============================] - 13s 35ms/step - loss: 0.0969 - accuracy: 0.9712 - top-5-accuracy: 0.9995\n",
      "Test accuracy: 97.12%\n",
      "Test top 5 accuracy: 99.95%\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(model):\n",
    "    \n",
    "    lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(lr_schedule)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"./tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "    \n",
    "    batch_size = 2000\n",
    "    num_epochs = 5\n",
    "    \n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "model_vit = create_vit_classifier()\n",
    "history = run_experiment(model_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "608dd712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 11s 34ms/step - loss: 0.0969 - accuracy: 0.9712 - top-5-accuracy: 0.9995 3s - loss: 0.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09692513942718506, 0.9711999893188477, 0.9994999766349792]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vit.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03d9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model_vit.predict(x_test[:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef3a69a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 3\n"
     ]
    }
   ],
   "source": [
    "pred_img_num = 200\n",
    "image = np.squeeze(x_test[pred_img_num,:,:,0])\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction:\", np.argmax(prediction[pred_img_num,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdb4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
